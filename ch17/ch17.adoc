== Storing multiple values per key

In the last chapter, we build a B+Tree and earlier in the book we implemented an Extendible Hash Table. When I built them, I stated that they are going to store a single `uint64_t`
value. 
In other words, if we compare them to in memory containers,  B+Tree is a `sorted_map<span_t key, { uint64_t value, uint8_t flags }>` and the hash table is a 
`map<uint64_t key, { uint64_t value, uint8_t flags }>`. That works great as long as we have a single value to store, but what happens if we want to have multiple values for 
the same key?

Let's consider a usage scenario for the B+Tree, we want to store a mapping between an IP address and the time of access. I other words, we want to use a B+Tree to implement the 
following function: `result_t record_ip_access(state_t* state, char* ip_address, time_t access_time)`. For the purpose of discussion, we'll ignore the different possible encodings 
of IP addresses and just focus on the code in <<record_ips>>.

[source]
[[record_ips]]
.Recording IP addresses
----
time_t now = 1600000000; // Sep 13, 2020

ensure(record_ip_access(state, "127.0.0.1", now - 60));
ensure(record_ip_access(state, "127.0.0.1", now));
ensure(record_ip_access(state, "127.0.0.1", now + 60));
----

As you can see in <<record_ips>>, we have a problem. We need to record _multiple_ values for the same key. Right now, the B+Tree code cannot handle this scenario at all. Each key
has a single value, that is all. So how can we store multiple entries for the same key? 

The answer, for B+Trees, is actually quite simple: We aren't going to. Yet we still want to allow the code in <<record_ips>> to work, so what should we do?

A key observation here is that we don't _need_ to store all the values in a single key. To be rather more exact, the key provided to us by the caller doesn't have to be the key we
use for the B+Tree. This is exposed by the API shown in <<btree_multi_api>>.

[source]
[[btree_multi_api]]
.`gavran/db.h` & `btree.multi.c` - API for working with multiple values per key in a B+Tree
----
// from gavran/db.h
include::../include/gavran/db.h[tags=btree_multi_api]

// from btree.multi.c
include::./code/btree.multi.c[tags=multi_search_args]
----

You can see that the API in <<btree_multi_api>> doesn't have a `set` function but an `append` one. We also don't have a way to `get` an item from a multi value, but we have to 
iterate to get all the values. The API surface is quite different, even if the underlying implementation isn't that different. 

Conceptually, what we are going to do is to change the key that the user provided us. So the three calls we have in <<record_ips>> would result in the following entries 

The idea is that instead of writing the key as the user supplied it, we'll extend it with a unique value. Here are the actual B+Tree entries after inserting three records:

* `127.0.0.1,1599999940` -> 1599999940
* `127.0.0.1,1600000000` -> 1600000000
* `127.0.0.1,1600000060` -> 1600000060

Because we have the `,1599999940` postfix to the key, we are able to store multiple values on the same "key". I'm actually using the value
of the key that the user specified as part of the actual key that is written to the B+Tree. The idea behind that is that this gives us a
simple way to create a set. 

=== Building a B+Tree to hold multiple values per key

The key `127.0.0.1,1599999940` is how we _think_ about the multiple values, but that isn't how we actually store them. The key is actually
built using the following manner: `key + byteswap(val)`. Let's see what we are actually doing here. We extend the key by 8 bytes and append
the value in big endian format. The reason we use big endian is to allow us to get natural sort order using `memcmp()` for the keys. 

I didn't use `varint` encoding here for a reason. If all the keys are extended by exactly 8 bytes, we can compare the actual size of the key
to the expected size of the key and ensure that we aren't confused a longer key with identical prefix. Because I'm using big endian format
for the values, we ensure that the final key that is written to the B+Tree is sorted.

.The conceptual data format for multi values in B+Tree
****
The usual B+Tree can be thought of as a `sorted_map<span_t key, { uint64_t val, uint8_t flags } >`. A B+Tree for multiple values, on 
the other hand, is more similar to `sorted_map<span_t key, sorted_set<uint64_t>>`. The idea is that we are going to allow multiple values
for the same key, and we are able to iterate over them in sorted order.

Note that we don't have a `flags` value here, because we are using the `flags` value in order to _implement_ multi values.

We are storing and retrieving the values in a sorted order mostly because this is a nice side affect of the implementation decision. It ends
up a good decision, because there are _many_ scenarios where getting the values in sorted order can be very helpful.
****

Let's take a look at <<btree_multi_append>> and see how thing work. I'm starting with this function because it allows me to showcase all the
ways that we handle multiple values in a single tree. 

[source]
[[btree_multi_append]]
.`btree.multi.c` - Appending a multi value to a B+Tree
----
include::./code/btree.multi.c[tags=btree_multi_append]
----
<1> Check the key and find what type of values are currently stored in the B+Tree.
<2> If we are using nested values, set the value in the nested tree.
<3> Add the value with uniquifier to the B+Tree.
<4> Move to nested mode if there are enough entries with the same key.

At the `btree_multi_search_entry()` at the start of <<btree_multi_append>> we allocate a buffer that is capable of holding the key size plus
eight bytes. This buffer is stored in the `args.buf` field and the key is copied to the start of the buffer. We search the B+Tree for other keys
with this prefix to figure out how we are supposed to write to the tree, more on that later.

I'm going to skip this now to look at the third point in <<btree_multi_append>>, we reverse the value and append it to the key and then set the
key in the B+Tree. Note that we mark the entry as `btree_multi_flags_uniquifier`, because the value of the entry is used to create a unique key for the
value. 

.Range scans in B+Tree
****
B+Trees are sorted. That is a very important aspect of how they work, but so far we haven't actually _done_ much with them. Here, we are going to start
using this property a lot. 

Because all the multi values for a particular key share the same prefix, we are able to search for them easily. First, do a lookup for the key that we
want to find, then find all the values that has the appropriate size and the same prefix. This is very cheap to do and the building block of many operations
on B+Trees. 
****

The downside of the uniquifier approach is that it duplicates a lot of information. The key `127.0.0.1` is 9 characters in size, and if we add the 8 bytes of
the value, we end up with 17 bytes for a key. Let's assume that we have to track a _lot_ of accesses for a given IP, what would be the overhead of multi values?

Well, at 17 bytes per entry (excluding the cost of the value), we are going to take over 16MB (!) of space just for the keys when we need to store a million entries
for a particular IP. The situation is worse with IPv6, where we've values such as: `2001:0dc5:72a3:0000:0000:802e:3370:73E4`. In this case, we'll use about 45MB of
disk space just to hold the keys with million records. I believe the official term for this kind of behavior is: "this sucks!".

The code in <<btree_multi_append>> is a lot more complex than simply appending the value to the key and storing that in the tree. That is meant to handle this exact
scenario. At a certain point, the overhead of duplicating the key grow high enough that we have to take another approach. This is actually the second point in 
<<btree_multi_append>>. 

If the `btree_multi_search_entry()` indicates that we are using a nested tree, we are going to take the value and store that as a _key_ (using `varint`) in the nested
tree. That can be confusing, so let's dig into what is going on here. At a certain point in time, we'll convert the keys + values that are stored as a flat list in the
root tree into a nested tree. 

That nested tree is used solely for holding the values of a particular key. In other words, we'll have a `127.0.0.1` key in the root tree, whose value would be a `tree_id`
that would be used to hold the values for that key. And because we need to store just the values, we can simply encode them using `varint` and store them in the _keys_ of
the nested tree. That will allow us to do scans and ordered iterations on the values associated with a key very cheaply. You can see the different formats in <<multi_formats>>.

.Comparing the different formats of multiple values per key
[[multi_formats]]
[cols="2a,2a", options="header"]
|===
| Key + value, flat list | Key -> `tree_id`, nested tree
| * `127.0.0.1,1599999940` -> 1599999940
* `127.0.0.1,1600000000` -> 1600000000
* `127.0.0.1,1600000060` -> 1600000060
[cols="2a,2a", options="header"]
| `127.0.0.1` -> `tree_id`  (see table below)
!===
! Key ! Value
! `1599999940` ! 0
! `1600000000` ! 0
!`1600000060` ! 0
!===
|===

One of the more important parts in `btree_multi_append()` is the call to `btree_multi_search_entry()`. This is where we find how we store the values for a particular key. Let's
look at how this is done in <<btree_multi_search_entry>>. 

[source]
[[btree_multi_search_entry]]
.`btree.multi.c` - Finding the appropriate entry in multiple entries 
----
include::./code/btree.multi.c[tags=btree_multi_search_entry]
----

The first step in `btree_multi_search_entry()` is to call to `txn_alloc_temp()`. To avoid the need to continuously
allocate and free small buffers, we have the `tx\->state\->tmp.buffer` field. Just like the `tx\->state\->tmp.stack` field, the idea is that we allocate this once for the lifetime
of the transaction and avoid having to call `malloc()` and `free()` all the time. You can see the implementation of this in <<txn_alloc_tmp>>. The temp buffer is freed when the transaction
is closed. Note that this is a transaction shared buffer. In other words, it is likely that it will be used by multiple parties and we can only assume that it isn't modified while
it is our control. We cannot assume that it will retain its value between calls to the public API of Gavran.

We copy the key that we are searching for to the temporary buffer and zero the remaining size (8 bytes). If the key that we are looking for is `127.0.0.1` (9 bytes) and the final key
we'll get is `127.0.0.1\0\0\0\0\0\0\0\0`. The last 8 bytes are zeroed. We try to match the first key that match this key or are higher. We use the cursor API we use for this will find
us _a_ key that is equal or greater to this key. It isn't necessarily a _useful_ key, though. For example, it may find us a key whose value is `127.0.0.2`, since that is greater than
`127.0.0.1\0\0\0\0\0\0\0\0`.

For that reason, we check that the key we find is valid. It has the right size and the prefix match to the key we are searching for. The next step is to check the `flags` of the entry.
A multi value entry should have either `btree_multi_flags_nested` or `btree_multi_flags_uniquifier` as it `flags` value. The `btree_multi_flags_nested` flag means that the `val` of 
the entry is the `tree_id` of a nested tree. And the `btree_multi_flags_uniquifier` flag means that we have added a the value as a uniquifier to the key. 

[TIP]
.Dealing with shared prefixes
====
One of the things that we have to worry about is what to do with shared prefixes in a multi value B+Tree. If one key is `127.0.0.1` and another key is `127.0.0.1\0`, for example. 
That is a _valid_ key, after all. 

We handle this appending a fixed size to all such keys and comparing the expected key size to the key that is actually in the tree. We also validate that the flags value of the
tree is set to one of the expected values before accepting it as a multi value key.
====

[source]
[[txn_alloc_temp]]
.`txn.c` - Allocating temporary buffer at the transaction scope
----
include::./code/txn.c[tags=txn_alloc_temp]
----

=== Using a nested tree to optimize key duplication for multiple values

Going back to <<btree_multi_append>>, we talked about the first three points. Checking an entry, adding an item to the nested tree and adding an entry with a uniquifier. The last
action in `btree_multi_append()` is checking if we have too many uniquifiers with `btree_convert_to_nested_if_needed`, shown on <<btree_convert_to_nested_if_needed>>.

[source]
[[btree_convert_to_nested_if_needed]]
.`btree.multi.c` - Checking if we have enough uniquifiers entries that it is worth to convert to a nested tree
----
include::./code/btree.multi.c[tags=btree_convert_to_nested_if_needed]
----

The code in `btree_convert_to_nested_if_needed()` simply scans through the tree searching for uniquifiers entries and count all of them. If we have more than 16 such entries, we'll
convert the entries into a nested tree using the `btree_convert_to_nested()` function.

Why do we have a limit of 16 uniquifiers entries before converting to a nested tree? A nested tree is going to take at least one page, so that would be 8KB. Using the IP example we
have been using so far, 16 entries would have a cost of just 272 bytes. That is a big difference from 8KB, so why make the change so early?

The cost of `btree_convert_to_nested_if_needed()` is linear to the number of uniquifiers items in the tree. The larger the number of uniquifiers entries we allow, the more expensive
it is to add a new entry and then scan the tree to count the number of items that match the shared prefix. We could think of a few ways to avoid this issue, of course, but they are 
fairly complex. 

The deciding factor, however, is that common distribution patterns we have for multi value entries:

* One
* Few
* Lots and lots and lots

When we have to deal with multiple values for a single key, if there are more than a few (which I explicitly don't define but is very small, in most cases) repetitions, we'll have _lots_
of repetitions. In that case, it is better to switch early to the more efficient manner of encoding multiple values.

With a nested tree, millions values for `127.0.0.1` will not take 16MB, they will take a maximum of 8MB and usually a lot less. The same will hold true for 
`2001:0dc5:72a3:0000:0000:802e:3370:73E4`. With millions of entries, we'll use exactly the same size as for `127.0.0.1`, since we only need to repeat the key once. 

.Integer encoding options
****
It is worth noting that the manner we encode the values in the nested tree isn't ideal for the purpose. We need a mutable data structure to allow for efficient appends and removal
from the set of values for a particular key. If we had better control over the access patterns, I might have chosen a different approach.

Another way to handle this scenario is to use Roaring Bitmap, which allows to efficiently encoding sets. https://github.com/lemire/streamvbyte[StreamVByte] is an integer encoding
scheme that allows to compress integers and decode them at a rate of 15GB/sec. The problem is that it expects to work in batch mode and isn't helpful if you have a lot of mutations
over the data set. There are other encoding formats such as `FastPFor`, which has better compression rate, but are slower to decode.
****

The actual conversion of the uniquifiers entries to a nested B+Tree is done in `btree_convert_to_nested()` and is shown on <<btree_convert_to_nested>>.

[source]
[[btree_convert_to_nested]]
.`btree.multi.c` - Converting the uniquifiers entries to a nested tree
----
include::./code/btree.multi.c[tags=btree_convert_to_nested]
----

In `btree_convert_to_nested()` we crete a nested tree (we'll look exactly how that works in a bit) and iterate over the root tree. We take all the values that has the key prefix
we are working on and write them to the nested tree. We write the _values_ as keys using `varint` encoding, leaving the actual `val` and `flags` zeroed. That is because we need to
track just the values associated with the key (the timestamps for the IP recorded, in our example). Using `varint` encoding gives us interesting advantages:

* We use less space than otherwise would be required.
* We keep the data in sorted order, which helps iteration.
* The keys are unique, which means that we have much easier time adding and removing items to the set.

You'll note that we are working in somewhat of a strange fashion in the `btree_convert_to_nested()` function. We have a `while` loop and we keep searching the root tree on each 
iteration. Why is that?

We are calling `btree_del()` on the root tree, which will _invalidate_ the iterator that we use. In order to get the next item, we have to search again to find the next entry that
match the query. As usual, the `btree_cursor_search()` find the first key that is equals or greater than the key that we are searching. The key we are using in `btree_cursor_search()`
is the key prefix plus 8 bytes of zeros, so we'll scan to find all the uniquifiers for a particular key. When we find a value that isn't a uniquifier or doesn't have the right size
and prefix, we'll complete the process.

The last action on `btree_cursor_search()` is to update the root tree with the `tree_id` of the nested tree with the `btree_multi_flags_nested` flag. Note that we use the `buf`'s key
value, which is the key prefix with 8 zero bytes appended. 

Creating a nested tree done in the `btree_create_nested()` function and is more involved than simply calling to the `btree_create()` function. Let's look at <<btree_create_nested>> to
see what is going on here.

[source]
[[btree_create_nested]]
.`btree.multi.c` - Creating a nested tree and wiring it for release when the root tree is released
----
include::./code/btree.multi.c[tags=btree_create_nested]
----

Most of the work inside of `btree_create_nested()` is done to add the newly create tree to the list of nested tree on the root tree. Why do we need that? This is required to make it
cheap to remove a tree using `btree_drop()` when it has nested trees. Otherwise, we would need to scan the whole tree and inspect all its data. By keeping a separate list of nested
trees, we have a much easier time handling this scenario. 
Let's look deeper into deleting items from a multi value tree.

=== Deleting a value from a multi value B+Tree

After looking into appending a value to a key, let's look at the other side, removing a value from a key. This is done using the `btree_multi_del()` function, shown on <<btree_multi_del>>.
Note that in this case, unlike the usual `btree_del()` call, the `val` field in the `btree_val_t` instance is important. This is the value that we'll remove from the key.

[source]
[[btree_multi_del]]
.`btree.multi.c` - Removing a single value from a key (and doing cleanup if needed)
----
include::./code/btree.multi.c[tags=btree_multi_del]
----

The code in `btree_multi_del()` starts by calling to `btree_multi_search_entry()`. If we couldn't find a matching value, there is nothing to do and we can return immediately. If there
_is_ a value, we need to check if this is a nested tree or a uniquifier value. If we aren't using a nested tree, we'll set the key buffer to the uniquifier value and remove it from the
tree.

If we _are_ using a nested tree, however, we have a more complex workflow. First, we remove the value from the nested tree keys, then we check if the nested tree is completely empty.
If it has any values, we'll leave it as is. In other words, if a multi value key has enough values to grow to a nested tree, it will not be downgraded to uniquifiers entries. It is 
only when the number of entries in the nested tree reaches zero that we'll remove the nested tree. Dropping the nested tree, which is done on <<btree_drop_nested>>. 

[source]
[[btree_drop_nested]]
.`btree.multi.c` - Removing a single nested B+Tree and unhooking it from the root tree's list
----
include::./code/btree.multi.c[tags=btree_drop_nested]
----

The process is pretty simple, we remove the nested tree that we are about the delete from the linked list of nested trees on the root page and then delete the nested tree directly.
I also had to change the `btree_drop()` code, which is shown on <<btree_drop_only>>.

[source]
[[btree_drop_only]]
.`btree.c` - When removing a B+Tree, also remove all its nested trees
----
include::./code/btree.c[tags=btree_drop_only]
----

This completes the process of adding and removing  multiple values to the same B+Tree, but there is still quite a bit of functionality on the table.

=== Iterating over multiple values per key using B+Tree

We have appended multiple values to a key in the previous section of this chapter. What we haven't done yet is read the data back. When using multi value trees, we don't have a `get`
or a `read` operations, we can only iterate over the values for a particular key. The iteration over the values of a key in our implementation has the following properties:

* If the value does not exists, you'll get an empty iteration.
* The values are iterated in ascending value order.

This hold true whatever you are using uniquifiers entries or a nested tree and isn't exposed to the outside world. We provide two API calls to allow iterating over the values of
a key. Let's start by looking into `btree_multi_cursor_search()`, shown on <<btree_multi_cursor_search>>.

[source]
[[btree_multi_cursor_search]]
.`btree.multi.c` - Creating a cursor to iterate over the values of a particular key
----
include::./code/btree.multi.c[tags=btree_multi_cursor_search]
----

We start by calling to `txn_alloc_temp()` to allocate a temporary buffer that we'll use to setup the actual multi value key in. This is a transaction shared value which is used
only for the duration of this function. We construct an internal cursor `it` that we use to search the root tree for the first entry that equals or is greater than our search key.
We validate that it is a proper prefix for what we want to search on and then we check whatever we are looking at a nested tree or not.

In the case of a nested tree, the iteration is very simple. We free the `it` cursor and direct the `cursor` instance we got to go over all the nested tree. We tell the `cursor`
to start the iteration at the beginning of the tree and the rest of the work would be simply iterating over a tree as usual. Note that we free the `it` cursor early (even though
we have the free registered in a `defer` call) because we want to avoid two concurrent cursors. That would force us to allocate and we can avoid it. 

For uniquifiers entries, on the other hand, we can search in the root tree. We go back one step (to undo the existing `btree_get_next()` call and move the ownership of the cursor
stack from the `it` internal cursor to the `cursor` instance that was provided by the caller. 

The `btree_multi_cursor_search()` merely setup the cursor, in order to actually iterate over it, you need to call to `btree_multi_get_next()`, shown on <<btree_multi_get_next>>.

[source]
[[btree_multi_get_next]]
.`btree.multi.c` - Getting the next value from a 
----
include::./code/btree.multi.c[tags=btree_multi_get_next]
----

The first thing we do in `btree_multi_get_next()` is to check if `has_val` is set to false. This can happen if there was no entry found for the key. It isn't _strictly_ necessary,
but I find that it makes it easier to understand what is going on here. 
We copy the user provided key to the side. This is because we are going to be iterating over the cursor and that would change the value in `cursor\->key`. We then step to the next
entry in the cursor. 

If we are iterating over a nested tree, we start the iteration placed just before the beginning, which means that we can just iterate over all the entries in the nested tree. As a 
reminder, the _keys_ in the nested tree represent the values that we stored for the key in the root tree, so we decode them to the `cursor\->val` field and we are done. 
When we are running over uniquifiers entries, we scan through the root tree and check that we are still on the same prefix as we are search on. In this case, the value in the cursor
is also one of the values on the key, so we can return that directly.

[IMPORTANT]
.Remember to call `btree_free_cursor()` on the multi value key cursors
====
Just like when you are using `btree_cursor_search()` and `btree_get_next()`, the cursor that we use for `btree_multi_cursor_search()` and `btree_multi_get_next()` _has_ to be freed
after usage using `btree_free_cursor()`. 
This makes sense, since we _are_ using `btree_cursor_search()` to implement `btree_multi_cursor_search()`. 
====

This concludes the work required to handle multiple values per key using B+Trees. We are going to extend the same API for handling multiple values for when we use our Extendible Hash
Table.

=== Building a hash that will hold multiple values per key

I also want to allow us to store multiple values when using a hash. Conceptually, I want to build a `map<uint64_t key, set<uint64_t> values>` collection. Each key in the hash table
may point to one or more values and we want to be able to append and remove to them at will. That turns out to be a somewhat tricker task. When adding multiple values to the B+Tree, 
I took advantage on the fact that I could _extend_ the key to add uniquifiers to the actual key we store in the B+Tree. With the hash table, there is no good way to mimic this approach. 
For that reason, using a multi value hash table is going to take cooperation between two data structure that we already built: the hash table and the container. 

The idea is simple. When we store a multi value in the hash table, we are going to use the `flags` to distinguish the types of values we have. The following options are allowed:

* `hash_multi_single` - The usual mode, in which the `key` and the `val` are stored directly in the hash table.
* `hash_multi_packed` - The `key` is stored in the hash, but the `val` points to an `item_id` which is a buffer that contain many values for that particular key.
* `hash_multi_nested` - The `key` is store in the hash, and the `val` is a `hash_id` of a nested hash table. Entries in the nested hash table has their `val` set to zero
  and the `key` actually hold the multi values for the root key. This is similar to the nested B+Tree from the previous section.

The API that we expose the the users is shown in <<hash_multi_api>>. Unlike the B+Tree API, the multi hash API isn't able to operate solely within the constraints of its root data
structure and needs assistance from an associated container. We'll see exactly why shortly.

[source]
[[hash_multi_api]]
.`gavran/db.h` - API for working with multiple values per key in a hash table
----
include::../include/gavran/db.h[tags=hash_multi_api]
----

You can see that this is similar to the multi value API for B+Tree, with the same behaviors. You can append a value to a key, delete a value from a key or iterate over all the values
of a key. It is interesting to note that multi hash is working in _conjunction_ with the container, it doesn't _own_ the container. This is important, since it means that a single
container can be used to handle several multi value hash tables, for example.
We'll start looking into the implementation of the multi value hash table in <<hash_multi_append>>.

[source]
[[hash_multi_append]]
.`hash.multi.c` - Appending a value to a key in a multi value hash
----
include::./code/hash.multi.c[tags=hash_multi_append]
----
<1> There are no entries for the provided `key`, so we can simple enter a value to the root hash table as usual, tagging it as `hash_multi_single`. 
<2> There is a single value in the hash table, if it is the same as the existing value, we can do nothing. Otherwise, we'll convert the value into a `hash_multi_packed` value using 
    `hash_multi_set_single()`. 
<3> Adding a value to an already `hash_multi_packed` value using `hash_multi_set_packed()`.
<4> Adding a value to a nested hash table using `hash_multi_set_nested()`. 

There idea in `hash_multi_append()` is to decide what is the relevant scenario that we run into and call to the appropriate function to handle that. Let's look at `hash_multi_set_single()`
in <<hash_multi_set_single>>, which will convert a single value into a pair of values.

[source]
[[hash_multi_set_single]]
.`hash.multi.c` - Move from storing a single value per key to holding two values for the value in a packed mode
----
include::./code/hash.multi.c[tags=hash_multi_set_single]
----

The code in `hash_multi_set_single()` is quite interesting. We create a buffer and write the existing and new values for the `key` to the buffer. We then create a _container item_ to hold
that buffer and store the `item_id` of that item in the hash table with a `hash_multi_packed` flag. The idea is that we can't use the hash table to store multiple values, but we can use 
the container to store those values for us and store a reference to it. 

If we had additional writes to the same key, the `hash_multi_append()` function will direct them to the `hash_multi_set_packed()` function, shown on <<hash_multi_set_packed>>.

[source]
[[hash_multi_set_packed]]
.`hash.multi.c` - Append a value to a packed set stored inside a container
----
include::./code/hash.multi.c[tags=hash_multi_set_packed]
----

The code in `hash_multi_set_packed()` will load the existing item's data using its reference `item_id` and scan it. If we have a new value, we'll need to extend the buffer to fit the new
value. This is done using the `txn_alloc_temp()` function that we are already familiar with. 
Note that we place a limit on the size of such packed structures. Once we go beyond 128 bytes, we'll move to a nested model. We'll look into that soon, for now, let's see what happens when
we add an item to packed value.

We allocate the new buffer, copy the existing data and write the new value at the end. Then we update the container's item and update the `item_id` if needed. The values in the packed 
buffer are not stored in any particular order and we have to scan through the whole buffer each time we append to it. Given that we are talking about a pretty small maximum size, I'm not
worried about this.

The size limit is 128 bytes, which _is_ small. The idea here, just like with the B+Tree move from uniquifiers to nested trees, is that the data distribution of values means that once you
go beyond a certain size, you are almost certain going to be growing a lot larger. For that reason, it is best to move to the more efficient model early.
We convert the value from packed value to a nested hash table in <<hash_multi_set_convert_to_nested>>.

[source]
[[hash_multi_set_convert_to_nested]]
.`hash.multi.c` - Convert a packed buffer of values into a nested hash table
----
include::./code/hash.multi.c[tags=hash_multi_set_convert_to_nested]
----

The code in `hash_multi_set_convert_to_nested()` will simply create a new nested hash table and add the values from the packed buffer (and the newly set value) to the nested hash table 
as keys. After we are done modifying both hash tables, we record the nesting association between them in the same manner we did with nested B+Trees. The parent hash table's metadata has
a linked list of associated hash tables. This is managed by the `hash_multi_write_nested_hash()` function, shown on <<hash_multi_write_nested_hash>>.

[source]
[[hash_multi_write_nested_hash]]
.`hash.multi.c` - Record the new hash table to the list of nested tables in the parent hash table
----
include::./code/hash.multi.c[tags=hash_multi_write_nested_hash]
----

We need to keep track of those nested hash tables to allow us to efficiently drop them when we drop the parent table. Otherwise, we'll need to scan through all the values that we have in
the hash table. By keeping track of the nested hash tables, we can directly drop them. 

The final piece of work we still have to review to support adding multiple values per hash key is what happens when we add a multi value when the key is nested. This is handled in the
`hash_multi_set_nested()` function shown on <<hash_multi_set_nested>>.

[source]
[[hash_multi_set_nested]]
.`hash.multi.c` - Add a value to a nested hash table
----
include::./code/hash.multi.c[tags=hash_multi_set_nested]
----

The isn't much in <<hash_multi_set_nested>>, since we simply utilize the existing API to store the new multi value in the nested table as a key. This conclude the exploration of how we
handle appending a new value to a multi hash. Let's look at how we handle the other side, reading items from a multi value hash table. Unlike the usual hash API, we don't have a 
function to get the value for a key. A single key may have any number of values, after all. For that reason, we have the `hash_multi_get_next()` method, which allows us to iterate over
all the values of a particular key. This is shown on <<hash_multi_get_next>>.

==== Iterating over multiple values per key

It is important to note that this iteration, unlike the one done using the iteration using `btree_multi_get_next()` isn't done in order. We are getting the values in an arbitrary order
that may _change_. In other words, you can't assume that two iteration of `hash_multi_get_next()` will return the results in the same order. There may have been an addition to the 
key which pushed it from packed to nested mode, which will change the order of iteration, for example.

[source]
[[hash_multi_get_next]]
.`hash.multi.c` - Going over all the values of a particular key
----
include::./code/hash.multi.c[tags=hash_multi_get_next]
----

The code in <<hash_multi_get_next>> is complicated by the fact that we need to handle quite a few different states and support re-entry to the function. We keep the state of the 
iteration in the `iter_state` structure inside the `hash_val_t`. The first thing we do in `hash_multi_get_next()` is check if we are iterating over a nested hash table using the
`iterating_nested` field. This is initially set to `false`, so we'll ignore it for now.

We get the relevant key from the hash table and check if it exists. If it doesn't, we'll stop the iteration. In other words, iterating over a key that doesn't exists will not 
return an error, just an empty set of results. If the value exists, we check the `flags` of the value to decide how to deal with it. For `hash_multi_single` we'll return the
current value and setup things up so the _next_ call will end the iteration.

For `hash_multi_nested` flag, we know that the value in the hash table is the nested table `hash_id`, we update the hash table we search for and set `iterating_nested` to `true`. 
Then we recurse into the function again. This time, since `iterating_nested` is set to `true`, the condition at the start of the function applies and we'll use the standard 
iteration process for hash tables on the nested hash table. It is important to understand that the work here is simply to find what the actual nested hash table is and then 
forward all the rest of the work to it.

The final option we have here is the `hash_multi_packed` flag. In this case, we have multiple values stored inside a container, which we handle by calling to 
`hash_multi_get_next_packed()`. You can see how that works in <<hash_multi_get_next_packed>>.

[source]
[[hash_multi_get_next_packed]]
.`hash.multi.c` - Iterating over a set of packed values
----
include::./code/hash.multi.c[tags=hash_multi_get_next_packed]
----

In `hash_multi_get_next_packed()` we grab the packed value from the container and then read the next one. We keep the position we are on in the packed value using the `pos_in_page` 
field. Once we have read past the size of the value in the container, we know we can stop the iteration process.

.Iteration efficiencies of multi hash values
****
The way I structured the code here probably leaves some performance optimizations on the table. We have to do a lookup on the primary hash table on each call to `hash_multi_get_next()`
for example. That isn't something I'm too worried about, though. In most cases, we are going to either end up with small number of items or a lot of them. In other words, I'm not 
expecting a lot of values to be packed. It is either going to be a single value or _many_ values, which means that we'll end up with a nested hash table.

The cost of iteration on the nested hash table is checking a single field on each call to `hash_multi_get_next()`. In practice, I expect the cost to be nil, especially once you take
into account branch prediction. Optimizing single and packed options, on the other hand, would require us to make the `hash_val_t` structure larger to carry more state. 
The `hash_val_t` structure isn't big enough that this would be a major issue, but I avoided going that route to reduce the complexity in the codebase.

I'll need to revisit that decision once I have some benchmark numbers.
****

Now that we know how to append and iterate over multiple values, there is just one last thing that we need to do, _remove_ a value from a multi value hash table. 

==== Deleting a value from multi value keys

Removal of a value from the multi value hash is very similar to looking it up. The process starts with `hash_multi_del()`, shown on <<hash_multi_del>>.

[source]
[[hash_multi_del]]
.`hash.multi.c` - Remove a value from a multi value hash table
----
include::./code/hash.multi.c[tags=hash_multi_del]
----

The `hash_multi_del()` has four options that it needs do deal with. If there are no values for the provided key, there is nothing to be done. This is the easiest option. If there
is a single value for the key, we can simple remove it from the hash table and we are done. Things gets more interesting with the other two options. If we have to deal with either
packed value or nested hash table. Let's look at how we deal with packed values first in <<hash_multi_del_packed>>.

[source]
[[hash_multi_del_packed]]
.`hash.multi.c` - Removing a value from a packed set of values inside a container
----
include::./code/hash.multi.c[tags=hash_multi_del_packed]
----

In `hash_multi_del_packed()` we are scanning through the values packed into a container item. If we find a value, we allocate a temporary buffer from the transaction and copy the
_rest_ of the value to the new buffer. Then we update the container item. If the new value is empty, we can delete the item from the container and the key from the hash table. Note
that updating the container's item may move it. In that case, we'll need to update the item id that we store in the hash table. 

It is important to note that we aren't ever moving from a packed mode to a single mode. In theory, we could do that, it is easy enough to detect when we have a single value and move
the key to a single mode. The reason I'm not doing that is that in almost all cases, the key is going to be shortly removed or appended to with more values. 

[source]
[[hash_get_entries_count]]
.`hash.c` - Getting the count of entries in a hash table
----
include::./code/hash.multi.c[tags=hash_get_entries_count]
----

We haven't looked yet on how `hash_multi_del()` deals with nested hash tables. In <<hash_multi_del>> you can see that we call `hash_del()` on the nested hash table, but we aren't 
done just yet. We need to check if the nested hash table is empty. We do that by calling to `hash_get_entries_count()`, shown in <<hash_get_entries_count>>. If the nested table is
empty, we can drop the nested table and remove the key from the parent table. 

Removing the key from the parent table is obvious, but dropping the nest table is a bit more complex. In addition to freeing the pages held by the nested hash table, we also need
to remove the nested table from the linked list of associated tables on the parent table. This is all handled in the `hash_drop_nested()` function, shown on <<hash_drop_nested>>.

[source]
[[hash_drop_nested]]
.`hash.multi.c` - Dropping a nested hash table and removing it from the parent table
----
include::./code/hash.multi.c[tags=hash_drop_nested]
----

The code in `hash_drop_nested()` will first update the linked list of associated hash tables to remove the dropped nested table. Then it will call to `hash_drop_one()` to do the
actual dropping of the nested hash table pages. But what _is_ `hash_drop_one()`? We haven't seen that function before. I had to rename the `hash_drop()` to `hash_drop_one()` to
allow us to handle associated hash tables. This is because we may want to call `hash_drop()` on a hash table that is used with multiple values. The new `hash_drop()` implementation
is shown in <<hash_drop_with_nesting>>.

[source]
[[hash_drop_with_nesting]]
.`hash.c` - Drop a hash table and all its associated tables
----
include::./code/hash.multi.c[tags=hash_drop_with_nesting]
----

The first item in the linked list is the parent hash table, so we'll simply scan from the given `hash_id` and remove all the nested hash tables in the list. The bulk of the work
is still done by `hash_drop_one()`, we simply added another function to orchestrate it. 

And now we are done. We have the ability to store multiple values per key on both B+Trees and hash tables. What we are left with is to look at the tests and see how we can _use_
this feature, but we'll do that in the next chapter.

=== Unit Tests

The tests we have for this feature are fairly simple. We append, iterate, remove and check the contents of the multi value key again. The nice thing about this feature is that there
isn't _much_ to it, making it straightforward to test and use.

[source]
[[tests15]]
.`test.c` - Testing using multiple values for B+Tree and hash table
----
include::./code/test.c[tags=tests17]
----