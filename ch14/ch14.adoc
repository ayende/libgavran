= Persistent data structures

We have spent _quite_ a bit of time on the foundation of Gavran, how we put bits to the disk in the proper manner, implementing transactions, ensuring that Gavran meets all 
the ACID properties, etc. We also dealt with some fairly advanced features, from supporting Transparent Data Encryption to implementing log shipping. Having a solid foundation
is critical for a storage engine and I made the conscious choice to have a strong separation between the data structures that Gavran will offer and _how_ it is implemented.

In some cases, storage engines make no distinction between the layers or have strong correlation between the external interface offered and the internal details of the data store.
LevelDB, for example, offers a single data structure, a key/value store, which is the only thing that it _can_ really offer, given how it is implemented. Gavran is built to be 
more generic in nature. That meant that we had to spend a lot more time building infrastructure, but now that we have a robust core, we can move forward with things.

As I'm writing this, the Gavran code base hovers just under 2,500 lines of code (excluding tests), which isn't bad at all, given what we have implemented. 
Even with just the foundation, there are critical functionalities that are missing. Multi threading support, cross platform support and performance work are just some of the items 
that pops to mind when I think about the remaining work. Nevertheless, we are going to shift our focus a bit and start building persistent data structures for Gavran. 

So far we have dealt strictly with pages and aside from the free space bitmap we didn't really do anything interesting with them. This part of the book aims to change that. We are
going to implement data storage containers as well as support for hash indexes and B+Trees. These terms may not mean much to you right now, but they will shortly become much 
clearer. 

We'll start with the most basic of tasks, storing and retrieving data from the database. What we have done so far is store data in pages and then get them back. That _works_, but
only as long as we work with page size data. We need better tooling to be able to actually store and retrieve data properly. 

I'm going to introduce and implement a few very important data structures:

* Persistent hash tables - using extendible hashing for storing data on disk.
* Raw data containers - allowing to store raw data and retrieve is by an opaque handle. 
* B+Tree - allowing to do point, range and prefix queries.
* Indexed tables - using all three previous items, we'll create a table and allow to run indexed queries on our data.

You might have noticed that I'm talking about the B+Trees last, which is quite odd. B+Trees are the bread and butter of databases and storage engines. B+Trees are often standing at
the very core of persistent technologies. There are _many_ variants of B+Trees, optimized for specific scenarios. If you are interested in learning more about B+Trees, I would
recommend reading the http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.7269&rep=rep1&type=pdf[Modern B+Trees], it does an awesome job covering the state of the art in
regards to B+Trees. 

Why am I keeping B+Trees for last, in that case? The answer is that I want to build Gavran in an incremental fashion and starting with persistent hashing gives me the chance to 
do a very gradual slope. Another reason is that I personally find persistent hash table to be an extremely elegant data structure. 

Raw data containers are a way to store some data to the disk and get an opaque handle that we can later use to retrieve it. That doesn't sound very useful, but together with 
persistent hashing and B+Trees, they allow us to create a feature that is much larger than the sum of its parts. The last chapter in this part will combine all other aspects
together and create a table with indexes.

Even if you'll never make use of Gavran, I think that reading through the process of building all the elements that are required to build such a feature would be tremendously 
useful. And now, without further ado, let's get hashing, persistently.

== Persistent hash table

|=== 
| Data Type    | Hash table (extendible)
| Write cost   | O(1)
| Lookup cost  | O(1), exact match only
| Data type    | Key: `uint64_t`, Value: `uint64_t`
| Maximum size | Roughly 32 TB
| Iteration    | Arbitrary order (unrelated to key values)
|===

We already implemented a hash table in this book. The `pages_map_t` is a hash table for storing `page_t` values using the `page_num` as the key. When talking about _persistent_
hash tables, on the other hand, we have to deal with something that is quite different. The typical manner in which hash tables are implemented in memory is to create an 
array to hold the values. When the load factor become too high, we'll re-hash the table to a greater array.
The `pages_map_t` API does just that, the `pagesmap_expand_table()` function does just that, and we spent a lot of Chapter 4 implementing it. 

What is the problem in applying the same approach when we are writing to disk? We can allocate a page to hold the values and double the size whenever the load factor is too
high, no? That would _work_, if you don't care about performance.

.Extendible hashing is a magic term
****
If you are familiar with the term extendible hashing, it is very easy to find a _lot_ of resources on it. If you are trying to find details about on disk hashing structure,
on the other hand, you're going to be sent into many wild goose chases. 
Persistent hash table is a term reserved for immutable in memory hash table, for example. And file based hashing will yield results about MD5 and SHA1. 

It was very frustrating to realize that in order to find information about the topic, you need to know what is the right term for it to even show up in searches. 
****

Consider what would be the impact of extending a hash table that is 4 GB in size, for example? Using typical rehashing techniques, we'll need to write a _lot_ of data, which is 
a pretty bad idea, all around. The solution is presented in the http://cgi.di.uoa.gr/~ad/M149/p315-fagin.pdf[Extendible Hashing-A Fast Access Method for Dynamic Files] paper 
from 1979 (although similar solutions date back to 1971, it seems). 

=== Extendible Hashing

The idea is that instead of trying to have a single flat array to hold all the data, we'll have a two tier structure. There is the directory, which we'll first look at to find
the possible location of a value and then there is the page which contain that value. If you are familiar with B+Trees, this may sound very similar to how they are implemented,
but unlike B+Trees, there is no possibility of multiple levels, there is only ever the _directory_ and the data. 

As usual with data structures, is is much better to look an image to make things clearer. <<ehash>> shows the structure of an extendible hash. 

[[ehash]]
.An extendible hash structure, showing the directory and 3 pages of values
image::ehash.png[]

In <<ehash>> you can see directory on the left. That is the key for this data structure. We have a directory that contains 4 elements and points to data that is
found in three separate pages. You'll note that both the directory and the pages has this strange `depth` notation, what is that about? 
Like any hashing system, we start by taking a key and translating that into a number. We use that number to then lookup the location of the actual value. With extendible 
hashing, we use the binary nature of the number for our advantage.

Let's say that we want to lookup the key `"abc"` in <<ehash>>. We'll first need to turn that into a number, let's say that the `hash("abc") = 11`, in binary, that means that
we have a value of `10 11`. With this number, we need to identify the location of the value. The `depth` value on the directory is `2`, which means that we can use the 
two rightmost bits of the value to find it. In other words, we compute: `num & ((1 << depth) -1)`. 

If you aren't familiar with bit fiddling tricks, this is a way to say: give me the rightmost `depth` bits from `num`. Let's say that we have `depth = 2` and `num = 9`. 
The binary representation of `9` is `10 01`. Applying the formula above, we'll get the rightmost bits and have `01` in binary representation, or the number `1`.
If you'll look at <<ehash>> you'll see that the the entry at position `1` points to a page. We can go into that page
and find all the value whose hash number have the same rightmost significant bits. And inside that page, we can look for the value using more straightforward means. 

On the other hand, if you'll look at the first and second entries on <<ehash>>, both of them point to a page that has a `depth` of `1`. Why the difference?

Extendible hashing works in the following manner. We start with a directory that has a depth of `1` and two branching pages. One for all the values whose hash number is even 
and one for all  the values whose hash number is odd. In other words, one of the pages is for the hash number that ends with `0` and the other for those that end with `1`. 
At this point, the directory and the two pages are all going to be marked with a `depth` of `1`. 

At some point, we'll write enough values to a page that we will fill it to bursting. There is no more _space_ to write. At this point, we'll _double_ the size of the directory
and _split_ the full page. This is what happened on <<ehash>>. The page with the odd numbers (ending with `1`) became full first and the odd page was split into two. One of
them for all the values that end with `01` and the other for all the values ending with `11`. The `depth` of the directory is set incremented to `2` and the `depth` of the 
split pages is also set to `2`. 

What will happen if the even numbers page become full? At this point, we'll need to split this page (one with `00` and the other with `10`), but we'll not need to double the
size of the directory. This is because the `depth` of the page is less than the `depth` of the directory. https://en.wikipedia.org/wiki/Extendible_hashing[Wikipedia] has 
great coverage of the algorithm, but I found the Extendible Hashing paper to be very readable. You might also want to look at the
http://www.csbio.unc.edu/mcmillan/Media/Comp521F14Lecture15.pdf[Hash Based Indexes] presentation, which explain this in detail, as well as some alternatives.

Once I understood how it works, I was amazed how this seemingly simple idea translated the problem from very hard to obvious in retrospect. 

There are other persistent hashing algorithms. We can simply select a fixed number of buckets and proceed from there. That has issues down the road, but it is very similar to
how an in memory hash table would work. This is also Linear Hashing, which I'm not going to discuss here. There is a good paper comparing Linear Hashing to Extendible Hashing,
however: https://www.csd.uoc.gr/~hy460/pdf/Performance%20comparison%20of%20extendible%20hashing%20and%20linear%20hashing%20techniques.pdf[Performance comparison of extendible hashing and linear hashing techniques].

Extendible hashing is shown to be faster, but the size of the directory may be a limiting factor in the paper. This paper was written in 1990, so 30 years ago at the time of 
this writing. The authors recommended using Linear Hashing when main memory is at a premium. That is 30 years ago, I have to repeat again. This advise is no longer relevant. 

Let's talk about the size of the directory for a bit. 
We are going to use `uint64_t` page numbers as the values in the directory, which means that a single 8KB page will be able to point to 1,024 pages, representing 8MB of data.
That means that to compute the size of the directory using this model, we can simply reduce the size of the data by an order of magnitude. A hash table taking 64MB will use
64KB directory, for example. A hash table with 128GB of data will use a directory with a size of 128MB, etc.

That assumes that we have to think about the directory as an array of `uint64_t`, that isn't necessarily the case. We can set the directory as an array of `uint16_t` values,
where each directory page will have its own base for the values in it. That means that a 8KB directory page will be able to refer to 32MB of pages holding the actual value.
It will force us to ensure that for each directory page, all its interior pages must reside within the same 512MB range (so they will share the same base page). Using the 
128GB example, that will reduce the directory size to only 32MB. 
Except... the hash tables that I envision are not likely to hit these sizes. 

I talked about the generic data structure up to this point, but what I would like to implement is a lot more focused. I want to build what is essentially a 
`map<uint64_t,uint64_t>`. In other words, a way to lookup an `uint64_t` by another `uint64_t` key, that is all. We aren't going to need to implement hashing of values, that 
is the responsibility of the caller, not the hash table. Two keys that map to the same hash cannot happen, since we are going to take `uint64_t` as the key and treat it as 
if it was already hashed. 

I expect that we'll mostly store page numbers in the hash table, so using a simple `varint` encoding, so I would expect to be able to store about a thousand key/value pairs
in a single page. In other words, with an 8KB directory and 8MB of data, we are actually going to hold about 8.2 million key/value pairs in the hash table. If we had a hash
table that held a _billion_ key/value pairs, we'll need about 8.5 GB of data for the values and 8.5 MB for the directory. A hash table that deals with 100 million entries
will have a directory that less than a MB in size. 

In short, I don't _mind_ the size of the directory. It is a very reasonable size even if I'm using `uint64_t` array as the internal data structure for the directory up to 
ridiculous number of entries in the table. And even when dealing with billions of records, a directory that is roughly the size of a single selfie isn't that big of a deal
in today's world. 

Okay, that is enough theory and discussion, let's settle down and start actually implementing the persistent hash table for Gavran.

=== Implementing a persistent hash table

The first thing that I want to discuss is the actual API that we'll provide for working with persistent hash tables. You can see that in <<hash-api>>.

[source]
[[hash-api]]
.`gavran/db.h` - The API we'll use to work with hash tables in Gavran
----
include::../include/gavran/db.h[tags=hash_api]
----

There are a few things that we need to discuss about this API. You can see that we're going to allow to create and drop a hash table. You'll usually only create such a hash
table once and keep it for very long time. This is similar to tables in a relational database, you tend to _not_ drop them on a whim. In order to reference a particular
hash table, you'll need its `hash_id`, which is passed via `hash_val_t` to all the operations that we can do on the hash.
The rest of the API consists of the usual CRUd operations and not really interesting in terms of design.

[TIP]
.The structure of a hash page
====
An extendible hash table is composed of two aspects. The directory, which we'll implement using an array of `uint64_t`. But what about the hash pages? These we are going to
implement in a very different manner. For now, I'm going to hide their actual usage behind internal API, but I'll discuss how they are built in detail later in the chapter.
====

We'll start by looking at how we create a new hash table, which you can see in <<hash_create>>. 

[source]
[[hash_create]]
.`hash.c` - Creating a new hash table
----
include::./code/hash.c[tags=hash_create]
----

There isn't _much_ being done in <<hash_create>>. We allocate a page, mark is as a hash page using `page_flags_hash` and set the `hash_id` to be the newly allocated page number.
In other words, we use the page number of the allocated page as the `hash_id` of the hash table. We also create a hash _page_, and not a directory. But the whole _point_ is to
use a directory. What is going on here?

An extendible hash table requires a minimum of three pages (24KB) to work. One page for the directory, one for the even keys and one for the odd keys. That is at `depth` of `1`,
of course. We expect to be able to fit about a thousand entires into a single page, and it seems a shame to waste three pages upfront. So I'm going to start the hash table as a
single hash page. We'll write to this page until we are no longer able to and then we'll split it, turning the original page into the directory. You can see how we can read from
the hash table in <<hash_get>>

[source]
[[hash_get]]
.`hash.c` - Reading a value from the hash table
----
include::./code/hash.c[tags=hash_get]
----

If we have a single page, we'll read it directly from the page. But if we have a directory, we'll use `KEY_TO_BUCKET()` to get the index of the bucket where the page number of
the key lives. Then we'll just check this page for the value. The fact that we can find the exact page we need in a single check is the most important aspect of the hash table.

[IMPORTANT]
.Why are we permuting the key?
====
In <<hash_get>> you can see that we call `hash_permute_key()` as one of our first steps. Why is that? 
The extendible hash is using the lowest bits in the key to find the relevant page. But what if we are storing values that have a pattern? Consider the case of storing
offsets into a file that are 8KB apart. The keys we'll use are `[0,8092,16384,24576]`. All of these values have _12_ lower zero bits. Let's assume that we have a whole
lot of them. That means that we'll need to split the directory 12 times before we can place them in different pages. 

In other words, if we'll store a few thousand such values, 
which should be stored in just 3 pages (two for the hash pages, one for the directory). But because of the directory splitting, we'll need to place them in no less than 
4096 separate pages. That is the difference between 24 KB and 32 MB.

Using the `hash_permute_key()` will mix the bits in the key in such a way that such keys will lose their pattern and become uniformly distributed. Here are the permuted
values:

* 0 -> 0
* 8192 -> 7383475855875536826
* 16384 -> 14766951711751073653
* 24576 -> 17372031019375746200

Just looking whatever the number is even or odd will tell you that we have good distribution of value in this case. I've taken the `hash_permute_key()` implementation
from https://gist.github.com/degski/6e2069d6035ae04d5d6f64981c995ec2[this Gist]. You can read more about is in the 
https://nullprogram.com/blog/2018/07/31/[Prospecting for Hash Functions] post. 
====

In the vast majority of cases, the directory page is going to be cached in memory, so we'll have at worst a single disk access. In practice, given the expected size of the table
I'll expect that the entire table will be in memory. That means that while we are working on a persistent data structure, we can usually consider the cost of an in memory mode.
The costs we have here are fairly negligible. Permuting the hash key and then doing a single lookup into the directory before jumping directly to the relevant page. We'll see 
how we deal with the data _inside_ the page shortly. 

=== Writing a value to the hash table

I now want to turn to a much more complex scenario, how do we put an entry into the hash table? Let's look at <<hash_set>> to start investigating this process.

[source]
[[hash_set]]
.`hash.c` - Adding (or updating) a value in the hash table
----
include::./code/hash.c[tags=hash_set]
----
<1> If we are still working on a small hash table, we can use the `hash` page directly, without any directory.
<2> Find the page that we should be putting the new entry into using the directory.
<3> Add the new entry to the matching page. This can fail if the page is full.
<4> If we set the entry successfully, check if this is a new entry or an update to an existing one and update the global `number_of_entires` on the hash table.
<5> If we can't set the entry, that means that the page is full and we need to split it.

In <<hash_set>> we have to deal with a few different scenarios. If the hash table is small and can fit on a single page, we can use that directly, no additional work is required.
If the hash table is big enough to have a directory, we find the page that would hold the new entry and place it there. If we were able to place the entry in its page, we check
if this is an update or a new value, and set the `number_of_entries` on the hash table accordingly.

We may fail to add the value to the page, however. This can be when the page is full. At this point, we need to _split_ the page and increase the size of the hash table. As you
can imagine, this is one of the more interesting pieces of the hash table. Before we get there, let's see how the easy parts go. <<hash_set_small>> shows how we add an item to
a small hash table.

[source]
[[hash_set_small]]
.`hash.c` - Adding an item to a small hash table or creating a directory 
----
include::./code/hash.c[tags=hash_set_small]
----

The `hash_set_small()` isn't doing much, simply adding the item to the page using `hash_set_in_page()`. The interesting part is what happens when we have no more room in the page.
At this point, we need to move from a single page to a real table, with a directory. This is done in `hash_create_directory()`, shown in <<hash_create_directory>>.
Once that directory is created, we call `hash_set()` again to add the value into the newly created directory.

[source]
[[hash_create_directory]]
.`hash.c` - Creating a directory from an existing page
----
include::./code/hash.c[tags=hash_create_directory]
----

The majority of the work in `hash_create_directory()` is done by `hash_split_page_entries()`, but there are some interesting pieces in it. When we split the current page, we are
going to create two new pages that will have some of the data that was previously in the page. Then we allocate a new page and set it as a directory for the newly allocated pages.
We _free_ the existing page and then set the `hash_id` to point to the new directory page. 

[IMPORTANT]
.The `hash_id` is not a stable value
====
Because the directory may need to grow and because we use `hash_id` currently as the page number of a single hash or a directory, that means that whenever we create or grow the 
directory, the `hash_id` will change. 

There are ways to avoid that, actually. We could add some indirection to the `hash_id` or attempt to reuse the pages and leave a marker pointer or any number of other alternatives.
I chose to simply allow the `hash_id` to change whenever the directory is touched because that is the simplest mode for writing the hash table. This is going to make _working_ 
with the hash table harder, however.

We are still at a _very_ low level point in the API and we'll provide a much nicer interface down the line, so I'm not going to worry too much about the ergonomics of the API at
this point.
====

Let's look into how we split a hash page into two separate pages. This is a fundamental operation for the extendible hash table and what allows us to easily expand the size of 
the hash table without expensive operations. The idea is that given a single page, we'll end up with two pages that contain some of its entries. So the amount of changes when we
hit a page full is bounded. You can see how this works in <<hash_split_page_entries>>.

[source]
[[hash_split_page_entries]]
.`hash.c` - Splitting a hash page
----
include::./code/hash.c[tags=hash_split_page_entries]
----

We allocate two pages in <<hash_split_page_entries>> and set them as `hash` pages with the provided depth. Then we iterate over the existing page and send the entries to the left
or right pages, depending on the relevant bit. This is important, because assuming we have uniform distribution (which is the point of `hash_permute_key()`), we can assume that 
about half the entires will go left and half will go right.
We don't _rely_ on that. This is why after the `hash_create_directory()` we call to `hash_set()` again, which may split the table _again_ if the destination page is still too full.

Finally, we set the page numbers in the `pages` array. When called from `hash_create_directory()`, we simply pass the buffer that we have for the directory directly. So the call 
to `hash_split_page_entries()` modify it directly. This is possible because there is nothing in the directory. We'll need to do better than that when we split a page on a full
directory. 

We already saw where this happen, in `hash_set()` if we have a directory already but the page we want to add to is too full. At this point, we'll call to `hash_split_page()` to
split the page (and potentially increase the size of the directory). We can see how this works in <<hash_split_page>>.

[source]
[[hash_split_page]]
.`hash.c` - Splitting a hash page
----
include::./code/hash.c[tags=hash_split_page]
----

The `hash_split_page()` function starts by testing if the page we want to split has the same depth as the directory. If that is the case, we'll need to double the size of the
directory. But if the depth of the page we are splitting is smaller than the depth of the directory, it means that we can simply update the pointers in the directory and we
don't need to change the directory size. 

The most interesting piece of code in <<hash_split_page>> is the way we update the directory pointers. We first call to `hash_split_page_entries()` and pass it the page we
want to split as well as a small array to hold the ids of the new pages. The code in the `for` loop find all the locations that we need to update in the directory, but it 
may not be obvious what is going on there.

Let's assume that we have a directory with a depth of 4, so the size of the directory is 16. We now need to split a page whose `depth` used to be 1. We compute the `bit`, which
is simply the size of the directory at that particular depth. However, the directory right now is of size 16, so we need to update multiple locations in the directory. We start
from the lowest location that the key can be placed on and then jump `bit` positions each time. When the `depth` started as 1, then `bit` will be `4` (1 << (1+1) == 4). That 
means that we'll update the entries from the first possible location of the key to any of the locations.

Let's assume that our key is `16384` and the result of `hash_permute_key()` is `14766951711751073653`. Masking that with `(bit-1)` gives us a value of 1. So we'll then scan
through the directory and update the following locations: 1, 5, 11, 15.
On each position in the directory we update, we need to check which page we should place there, whatever it should be the left or right pages from the `hash_split_page_entries()`
call. We determine that by checking the relevant `depth` bit in the index. 
I think I mentioned before how _elegant_ extendible hash table are, didn't it? It doesn't take a lot of code but it result in an amazing behavior. 

The final stage in `hash_split_page()` is to free the page we split and we are ready for a new call to `hash_set()` that will use the modified directory. Again, in some pathological
cases, we may need multiple `hash_split_page()` rounds to make sure that we have enough space for a new value, but those should be _very_ rare.

The last part that we still need to look at with `hash_set()` is how we expand the size of the directory. This is done by `hash_expand_directory()` and is shown in 
<<hash_expand_directory>>.

[source]
[[hash_expand_directory]]
.`hash.c` - Doubling the size of the directory
----
include::./code/hash.c[tags=hash_expand_directory]
----

In `hash_expand_directory()` we start by allocating a new directory, twice as large (in pages) as the old one. And then we copy the old directory to the new buffer _twice_.
This means that we doubled the size of the directory, but we didn't modify any of the mapping. That part is left for the caller in `hash_split_page()`. 

[NOTE]
.We _always_ allocate a new directory
====
We'll always allocate a new directory in `hash_expand_directory()`. There are many cases (until the hash table reach 8MB in size) that we could simply
expand the table in place, since we already have the space in the page allocated to the hash table.

I chose not to do that to ensure that we'll always change the `hash_id`. If this is something that happens often, we'll know that we need to deal with it. Otherwise,
we may accidentally assume that the `hash_id` is stable after calls, which will be _usually_ true, until we had a bad bug. Start as you mean to continue, basically.
====

We have now explored all the pieces of adding a value to the hash table, let's look at the other side now, how can we _delete_ a value from the table?

=== Deleting a value from the hash table

Deleting a value from the hash table is very similar in concept to adding one. When adding a new value, we had to handle a full page, which meant that we had to split it.
When deleting an item, we need to consider how to _merge_ pages. We don't want to be too eager about it, otherwise certain workloads will cause us to do splits and merges
often. On the other hand, we want to be able to merge the pages (and reduce the depth of the directory) when we delete data to release the space we allocated to the table.

Deleting a value from the hash table stats in `hash_del()`, shown in <<hash_del>>.

[source]
[[hash_del]]
.`hash.c` - Deleting an item from the hash table
----
include::./code/hash.c[tags=hash_del]
----

The structure of the code in <<hash_del>> should be familiar by now. We have one part that handles the case where we have a single page in the hash table and we have the
other case where we have a directory in place. In the first case, we can remove the value directly using `hash_remove_from_page()` while in the later case, we have to 
first find the appropriate hash page and then call `hash_remove_from_page()` on it.

[QUESTION]
.Does the single page `hash` page optimization pay off?
====
In each of the high level hash table functions so far, we have to deal with two cases. One where the hash table is on a single page and we skip using a directory and one
where we have a directory and we need to jump to the actual `hash` page. 
That adds some non negligible amount of work and complexity to all of those function. The question is, is it worth it?

I think that it is worthwhile to do so. A single hash table page should contain between 3,500 - 1,000 entries, depending on the exact values that we'll store in the hash
table. That is big enough that many use cases will be able to use just a single page and benefit from the simpler code structure and reduction in space usage.
====

The most interesting thing in `hash_del()` is the very last call, `hash_maybe_merge_pages()`. Just like we have to handle page splitting when we add an item to the table, we
also have to deal with the other side, merging of pages when there isn't enough data in them. You can see how that is implemented in <<hash_maybe_merge_pages>>.

[source]
[[hash_maybe_merge_pages]]
.`hash.c` - Checking if we can merge the page with its sibling after a delete
----
include::./code/hash.c[tags=hash_maybe_merge_pages]
----
<1> We check if the page we deleted from and its sibling (the page that is one bit off from the current page) _together_ have will fill less than 75% of a single page.
<2> If we have just two pages in the hash table, we can drop down to a single `hash` page, instead of using a directory.
<3> Merge the two pages into a single one and update the directory.
<4> Shrink the directory if we can.

In <<hash_maybe_merge_pages>> we start by checking the _sibling_ page. That is the page that is off by one bit from the page we just deleted an entry from. The sibling page
is the target for merging. We check the used size on the page and its sibling and if they are more than 75% of a single page, we do nothing. Splitting and merging pages is 
not something that we want to do casually, so we give ourselves some buffer before we'll trigger a merge. 

Once we have determine that we have a good candidate for merging, we check the size of the hash table. If we are merging the last two pages, then we want to drop down to a 
single page `hash`, instead of using a directory. This is handled in `hash_convert_directory_to_hash()` and shown in <<hash_convert_directory_to_hash>>.

[source]
[[hash_convert_directory_to_hash]]
.`hash.c` - Merging the last two pages in a directory into a single `hash` page
----
include::./code/hash.c[tags=hash_convert_directory_to_hash]
----

In <<hash_convert_directory_to_hash>> we wipe the directory page (and its metadata) and turn that into a `hash` page. Then we start iterating over both of the pages we want
to merge and add them to the new `hash` page. We finish the function by freeing both pages while the directory page remains in use, but it is not have been recycled to be a 
`hash` page. 
In other words, we have implemented both grown and shrink on the table so we can grow from a single `hash` page to a directory and then shrink back to a single `hash`.
The process for merging two pages while still using the directory is very similar and can be seen in <<hash_merge_pages>>.

[TIP]
.Locality of reference in the hash table
====
Throughout this chapter, you might want to pay attention to the calls to `txn_allocate_page()`. In particular, the last argument we pass to it is a hint on where we would
_like_ to have this page. We talked about this when we built the function in Chapter 5 but now you can see it in action. Aside from the first call to `txn_allocate_page()`
in `hash_create()`, we always ask the lower levels of the API to allocate a page nearby the directory page or the current `hash` page. 

That means that most of the time, the pages are going to be nearby and can benefit from coarser grained reads from disk as well as have the chance to trigger read ahead 
optimizations by the operating system. 
====

[source]
[[hash_merge_pages]]
.`hash.c` - Merging two pages into a single one and updating the directory mapping
----
include::./code/hash.c[tags=hash_merge_pages]
----
<1> The actual merge is done inside `hash_merge_pages_work()`.
<2> Update the indexes in the directory to point to the new page
<3> Release the old pages, now no longer needed.

In <<hash_merge_pages>> we allocate a new page to hold the combined entries from the pages we merge. Note that the `depth` of the new page is the _minimum_ `depth` from both
pages. We call to `hash_merge_pages_work()` to do the actual work of merging the value and then we need to update the references in the directory. This is just the inverse
of the work we have done in `hash_split_page()` and follow the same principals.

The only thing that we still need to discuss is how to handle the process of shrinking the hash table directory. Let's look at the code in <<hash_maybe_shrink_directory>> and 
then we'll talk about how it works. 

[source]
[[hash_maybe_shrink_directory]]
.`hash.c` - Checking if we can shrink the directory and shrink it if we can
----
include::./code/hash.c[tags=hash_maybe_shrink_directory]
----
<1> Early exit if we found a `hash` page whose `depth` equals to the current global `depth`, indicating that we can't shrink the directory.

In <<hash_maybe_shrink_directory>> we start by scanning the metadata of all the pages in the directory. Note that we only touch the _metadata_ of the pages and that we have
set things up so it is _very_ likely that the pages are clustered together and reading a single metadata page will net us many metadata entries in a single read from the disk
(if the page isn't already in memory).

We check the `depth` of all the pages in the hash table and if _all_ of them are below the global `depth` (which is stored in the _directory_ metadata), we can shrink the table.
That is done by allocating a new directory page and copying just half of the directory over to the new page. The `hash_merge_pages()` call has already set things up so both halves of
the directory are identical at this point. 

We _could_ try to avoid allocating a new page on the directory shrinking, but the same logic that made me do it always on expansion holds here as well. I want to make sure that 
calling code is ready for the `hash_id` to change.

=== Iterating over the entries

Iteration over the the hash table turns out to be surprisingly tricky. If we have a single page, we already saw that it is a simple matter of calling to `hash_page_get_next()`,
even if we didn't look into how that is implemented yet. But when we have a directory, we have to deal with a problem. The directory can contain multiple references to the same
page. If you'll look at <<ehash>>, you can visualize the problem easily. We want to get every entry in the hash table _once_, but if we simply scan through the directory we'll
encounter some pages more than once and may return duplicated results. In order to handle this, the `hash_get_next()` function accepts a `pages_map_t` argument that will hold
the pages that were visited in previous calls to the function. You can see how that looks in <<hash_get_next>>.

[source]
[[hash_get_next]]
.`hash.c` - Checking if we can shrink the directory and shrink it if we can
----
include::./code/hash.c[tags=hash_get_next]
----

Iterating over a single `hash` page is easy enough. We'll see exactly how that works later in this chapter. The most interesting parts of this function is how we handle iteration
in the directory. Between invocations of `hash_get_next()` we remember the page that we were last iterating using the `it\->key`. 

There is a small trick here. We require that the  `hash_val_t` we accept in this method will be zeroed on the first call (except for the `hash_id`, of course). 
When we permute a zero, we'll accept a zero. So the first call will always go to index `0`, which is exactly what we want. There is also the `iter_state` we have in `hash_val_t`,
which is used to remember the iteration position _inside_ a page. 

Once we are done scanning a page, we'll add it to the `state` pages map and run through the directory, finding the next page that we haven't seen. It is the caller's responsibility
to create and destroy the page map for the call. 

This is almost all of it, we have implemented an extendible hash table that can manage to store `uint64_t` keys and values. It knows how to expand and contract and will allow us 
to find values with an `O(1)` complexity. We are able to iterate over the values, delete, create and set them. What we are missing, however, is everything about how we handle the
data _inside_ the page. So far we dealt exclusively with the directory and how the extendible hash table work. All the work about the page itself has been hidden behind our API. 

=== The structure of a `hash` page

Up until now, we look at the hash table from a high vantage point. We looked at the directory and how we manage the pages inside the table. In this section, I want to dive into the
structure of the data inside a `hash` page. The extendible hash table we build here hold entries of `uint64_t` keys and values. Using the most naive option, each entry would then
cost us 16 bytes to hold. In other words, a single `hash` page would be able to store up to 512 entries. 

You might have noticed earlier in this chapter that I kept referring to the `hash` pages as capable of holding much more than that, but I didn't give an exact number. This is because
we aren't going to store the entry data as is. We are going to use `varint` encoding for this purpose. This is a common encoding format for integers that allow us to pack small
integers with high degree of efficiency. The idea is that we can write an `int64_t` in far fewer bytes. The `varint` encoding is _really_ common and is used in many scenarios.
You can https://developers.google.com/protocol-buffers/docs/encoding[read more about the `varint` encoding] if you are unfamiliar with it. 
The API Gavran uses for working with them is shown in <<varint_api>>.

[source]
[[varint_api]]
.`gavran/internal.h` - Variable length integers API
-----
include::../include/gavran/internal.h[tags=varint_api]
-----

The `varint_encode()` function return the pointer it received _after_ the write of the number they got. The same is the case for `varint_decode()`. I'm not going to dive into 
the actual implementation. You can check the source code for that or read about the encoding format. What is important to understand about `varint` is that instead of taking 8 bytes
to store an `uint64_t` we encode the data so it will take between 1 - 10 bytes. The size of the encoded `varint` depends on the value we encode. The bigger the value, the more bytes
we need to encode the number. Note that we'll only cross the 8 bytes threshold if the value is larger than `72,057,594,037,927,936` so up until that point, this is an absolute win.

The `varint` encoding allows us to store the entries in a far more efficient manner, but it come with a different problem. Now that the size of the data isn't fixed, how can we find
where the relevant entry is. If we were storing the entries in the page as is, we could treat the page as an array with 512 elements and index into that directly using the provided
key. We already saw how well that works when we built the `pages_map_t` API.  How would we deal with the problem when working with variable size data?
Let's take a look at <<hash-page>> which shows the structure of a `hash` page.

[[hash-page]]
.The internal structure of a `hash` page
image::hash-page.png[]

The `hash` page is divided into buckets that are 64 bytes each. For an 8KB page, that means that we have 128 buckets inside the page. For each page, we have 1 byte header and 63
bytes that can hold data. we utilize the `hash_bucket_t` to work with each bucket, shown on <<hash_page_decl>>. 

[source]
[[hash_page_decl]]
.`hash.c` - Declarations used when working with the internal structure of a `hash` page
----
include::./code/hash.c[tags=hash_page_decl]
----
<1> Whatever this bucket is part of an overflow chain, using a single bit field.
<2> The number of bytes used in this bucket, stored as a 7 bits field number (range of 1 .. 128).

The code in <<hash_page_decl>> is interesting because it is using bit fields. Those are instructions to the C compiler that the fields in the index take _less_ than a single byte.
We use this to make sure that the header will only use a single byte, leaving the `data` field with 63 bytes for us to work with.

.Why use 64 bytes size for the hash buckets?
****
In most modern processors, the cache line size is going to be 64 bytes in size. I'm not going to go into details about cache lines but you might find this
https://meribold.org/2017/10/20/survey-of-cpu-caches/[Survery of CPU Caches] to be an interesting read. What is important to understand about the CPU will not go to memory every
time we need to read some data, it will load that into its cache (L1, L2, etc). The topic is _quite_ complex, so I'll leave it at that.

The majority of the cost is fetching the data from RAM to the CPU cache, multiple accesses on the same cache line is going to be effectively free. For that reason, having a 64 bytes
bucket (which is also aligned on 64 bytes boundary) means that each bucket is going to be its own cache line and multiple operations on that are going to enjoy very high speed.

Beyond that, in some cases we will want to access the buckets in a sequential order (when we handle overflows). This will also benefit greatly from the read ahead optimizations
done by the CPU because we use a very predictable pattern.
****

I think that the best place to start showing how the data is stored in a `hash` page is to see how we _read_ the data back. This is because the read doesn't have to deal with
any of the complexities we have to work with when writing an entry to the page. You can see how that works in <<hash_get_from_page>>.

[source]
[[hash_get_from_page]]
.`hash.c` - Get an entry from a `hash` page
----
include::./code/hash.c[tags=hash_get_from_page]
----

In <<hash_get_from_page>> we start by computing the initial bucket location. This is done by simply stripping `depth` bits from the `hashed_key` and using that to index into
the `buckets`' array. This is fairly standard behavior for a hash table and we saw very similar code in the page map. What is interesting is what we do in each bucket. We 
scan through the bucket's data (from the start to `bytes_used`) and call `varint_decode()` twice. Once to get the entry's key and once for the entry's value. We compare the 
key from the entry to the expected key and decide we have a match or not.

If we read all the entries from a bucket and didn't find the key, we have to make a choice. Should we continue to scan the _next_ bucket, to see if we had an overflow and had
to move the entry from its native position to another one? 
If the bucket we scan is marked as an overflow one, we will go ahead and scan the next bucket in line. Note that the `idx` variable may wrap to the beginning of the `buckets`
array if we are starting near the end of the array.

Another important consideration is that we'll only allow to scan up to `HASH_OVERFLOW_CHAIN_SIZE` buckets (which is set to 16). That means that we'll scan at most 1KB in 
the page if we can't find the value. The way we setup the structure of the `hash` page, we will compute the index from the `hashed_key` (excluding the `depth` bits, which are
identical for all the entries in the page). That gives us the ideal location for the entry, but we allow it to be shifted by up to 16 buckets from that position. If the entry
cannot be found within that range, it means that it isn't on the page at all and we can say that the value isn't in the hash table.

Iterating over the values in the `hash` page also take advantage of the buckets in the page, as you can see in <<hash_page_get_next>>. 

[source]
[[hash_page_get_next]]
.`hash.c` - Iterating over all the entries in the `hash` page
----
include::./code/hash.c[tags=hash_page_get_next]
----

The idea in <<hash_page_get_next>> is that this is a function that is going to be called multiple times. We use the `iter_state` field in the `hash_val_t` to hold the state
of the iteration in the page between invocations of `hash_page_get_next()`. The `iter_state` holds the position of the _next_ byte that we need to read.
We continue to do so until we run out of buckets in the page. 

In <<hash_get_next>>, where we iterate over the whole hash table, you can see that we set the `iter_state` to zero when we move between pages. That ensure that we scan through
each page from the start all the way to the end.

Now that we understand how the data is structured on a `hash` page, let's see how we _add_ the entries to the page.

=== Adding an entry to a `hash` page

The API we provide to add an entry to a page is the `hash_set_in_page()`, shown in <<hash_set_in_page>>. This function has to deal with adding an item to the page or updating
an existing value. 

[source]
[[hash_set_in_page]]
.`hash.c` - Adding a new entry or updating an existing entry in the `hash` page
----
include::./code/hash.c[tags=hash_set_in_page]
----

We start `hash_set_in_page()` by encoding the key and the value into a buffer. We use a `20` character buffer because that is the maximum number of bytes that two `uint64_t` 
values will take. Most often, the actual used buffer is going to be much smaller. We then scan the page to see if there is already an entry with the provided key. This is 
similar to the code in `hash_get_from_page()` code. We also check if the existing value and the new value are the same, in which case we have to do nothing and can return
immediately.

If the entry we found has a different value, we call to `hash_try_update_in_page()` to complete the update. Note that this may _fail_, if the new entry is larger than the 
previous one and there is no more space in the page.
If we scanned through the possible buckets in the page and couldn't find the entry, we can append it to the page using `hash_append_to_page()`, certain that it is a new value.

Let's look first in more depth into `hash_try_update_in_page()`, to see how we are trying to update the value. You can see the code in <<hash_try_update_in_page>>.

[source]
[[hash_try_update_in_page]]
.`hash.c` - Updating an existing entry's value inside 
----
include::./code/hash.c[tags=hash_try_update_in_page]
----

If the size of the old entry and the size of the entry match, we can simply overwrite the old entry and be done with it. That is the simplest scenario and the first that we
handle in `hash_try_update_in_page()`. If that isn't the case, we call to `hash_remove_in_bucket()` to remove the entry from the bucket. We do that using `memmove()` by overwriting
the range of the old entry and updating the `bytes_used` on the bucket. 

We then attempt to add the new entry buffer to the same bucket. This may fail, because the bucket it full. If that is the case, we exit the function and the caller will then
proceed to call to `hash_append_to_page()`. It is important to understand that the `hash_try_update_in_page()` will _remove_ the old entry in all cases, so failure to add the new
entry will have the side affect of removing the old one. Then we handle all the rest of the work needed to handle appending the new entry in `hash_append_to_page()`. You can see
the code for appending a new entry to the page in <<hash_append_to_page>>.

[source]
[[hash_append_to_page]]
.`hash.c` - Append a (known new) entry into the page, marking overflow buckets as needed
----
include::./code/hash.c[tags=hash_append_to_page]
----

The code in <<hash_append_to_page>> is where the most interesting work of the `hash` page is happening. There isn't a lot of code, but it does have some interesting aspects. Let's
look into that in detail.

We start from the ideal bucket location for the value and we check if we can place the value there. If the bucket is too full, we mark it as an overflow bucket and check the next 
bucket in line. The reason we need to mark the bucket as overflow is so operations such as `hash_get_from_page()` and `hash_set_in_page()` will know how far to scan. It is important
to note that the size of the `HASH_OVERFLOW_CHAIN_SIZE` means that we may place an entry a maximum of 512 bytes from its ideal location. If all the buckets in the allowed range are
full, we fail the append operation and return `false`. Remember that if we had an existing value before the call, it has been removed. 

We saw what happens when the `hash_set_in_page()` function returns false in <<hash_set>>. If we fail to place the value in the page, we'll split the page and try again to set the 
entry after the split. 

.Why do we need the `HASH_OVERFLOW_CHAIN_SIZE` limit?
****
The `HASH_OVERFLOW_CHAIN_SIZE` is set to `16` and it used to limit the number of overflow offset in the `hash` page. In other words, it it used to control how far from its ideal
bucket will we allow a value to be placed. The idea with this value is that it controls the space vs. time tradeoff in the `hash` page. The bigger the value, the more chance
we have to find a bucket with enough space to place the entry.
However, the bigger the value, the more memory we have to scan to _find_ the value. 

I run tests to find the sweet spot for this value. When setting the chain size to 1, the time it took to find a value was 0.54 s per operation, but the size of the hash table
with a million entries was 147MB. This is fairly obvious. As soon as a single bucket is full, we'll need to split the page. 

On the other hand, when setting the maximum size of the chain to be 128 (the maximum number of buckets in the page) the size of the hash set was 32MB, but the time to search 
soared to 0.77 s per operation. In this case, we'll scan the _entire_ page to find if the value is there or not. It may seem like a silly issue, but scanning 8KB to find a value
can be quite expensive.

When using a chain size of 8, the size of the hash table was 36MB, but the cost in time was 0.32 s per operation, _less_ than when we had a chain size of 1. Probably because we 
have almost as good locality of reference as we have when we can the whole page. 

What is probably the _ideal_ value is 16. The size of the data is 32.2MB and the operation speed is 0.33 s per operation. I'm mentioning these numbers because this is probably
something that we may want to pay attention to when we get to writing benchmarks for Gavran. 
****

=== Deleting a value from a `hash` page

Deleting a value from a `hash` page should be easy. We already saw how that works in <<hash_try_update_in_page>>, using the `hash_remove_in_bucket()` function. The problem with
relying on `hash_remove_in_bucket()` alone is that it will not shorten the overflow chains. That means that if we had a nearly full page and we start removing entries, we'll not
move the existing entries to their ideal bucket and have to still scan through more of the page to find the values. 

The code for handling deletions in a page is shown on <<hash_remove_from_page>>. It follow the same pattern of `hash_set_in_page()` and should be quite familiar by now.

[source]
[[hash_remove_from_page]]
.`hash.c` - Deleting an entry from a `hash` page and compacting the page if needed
----
include::./code/hash.c[tags=hash_remove_from_page]
----

We scan through the buckets where our entry may reside and if we find it, we call to `hash_remove_in_bucket()` to do the actual removal. The interesting bit about this code is
that if the bucket that we removed an entry from is an overflow bucket, we need to scan the _next_ buckets to see if we can move entries that are outside their ideal location 
to their proper place. This is done in `hash_compact_buckets()`, shown in <<hash_compact_buckets>>.

[source]
[[hash_compact_buckets]]
.`hash.c` - Compacting the buckets on a page, placing entries in their ideal location if we can
----
include::./code/hash.c[tags=hash_compact_buckets]
----

We start `hash_compact_buckets()` by scanning from the current bucket forward, and trying to find how many overflow buckets we have to deal with. Remember that we start from
a bucket that we just removed an item from, so we need to scan a maximum of 16 buckets forward, since that is the maximum distance that a value from that location can be placed on.

We then start from the furthermost bucket from our original page and see if it has any entries that should reside in a previous page. If we were unable to move an entry from the 
bucket to the previous location, we can't cut the overflow chain, but we still want to move the entries as far back as we can, to reduce the overhead when we scan for entries.

The reason we start with the furthermost overflowed bucket is that we can remove the `overflowed` marker from a bucket if we removed all the entries that don't belong to the
_next_ bucket _and_ the next bucket isn't marked as `overflowed` as well. If the next bucket is also overflowing, there may be shifted entries that exists in buckets beyond the 
range that we are scanning. 

Unfortunately, this is pretty complex, and I was unable to reduce the complexity further than what is shown in <<hash_compact_buckets>>. The good news is that this is limited to
a single page and the complexity doesn't leak out to the rest of the code.

=== Dropping a hash table

The last step that we need to handle before we can call a close to our chapter on hash tables is how we'll deal with _dropping_ the hash table. How can we make sure that all 
the data that is held by the hash table is released and all resources are freed. As usual, we have to consider two separate scenarios. The first is when we have just a single
`hash` page, in which case we can simply free it directly. But what happens when we want to drop a hash table that has a directory and multiple `hash` pages? You can see
the `hash_drop()` implementation in <<hash_drop>>.

[source]
[[hash_drop]]
.`hash.c` - Dropping a hash table and releasing all the pages associated with it
----
include::./code/hash.c[tags=hash_drop]
----

For a directory, we can simply iterate over the pages in the directory and release them one at a time. The only interesting thing we have to deal with in this case it to make sure 
that we aren't going to release the same page twice. We handle that by using a `pages_map_t` to hold the pages we already freed. 

.It is probably okay to free twice
****
I'm avoiding calling `txn_free_page()` twice on the same page because it isn't the right thing to do, but with our current implementation, freeing a page will have the following
affects:

* The page and its metadata will be zeroed.
* The page will be marked as free in the free space bitmap.

We are fine with both of those things happening multiple times, since the end result is going to be identical. I'm avoiding doing this because this is our _current_ implementation
and I don't want to create subtle dependencies between pieces of our code.
****

With the `hash_drop()` explored, we are done. We have a persistent hash table that will work with our transactional storage, we can create and drop it, add and remove values and
everything will just work. Or at least, so I assume. We still need to write some tests to verify this...

=== Unit tests

This has been a _long_ chapter. The extendible hash table is a beautiful data structure, but even with reducing the complexity and only handling `map<uint64_t, uint64_t>`, we
have about 700 lines of code added to Gavran, excluding the tests. I'm pausing to comment on this because we have to do deal with the associated complexity. One of the best
ways to handle that is to write code specifically to deal with this complexity.

I haven't shown it in this chapter, but in addition to the extendible hash code, we also have the `hash.debug.c` file, which accounts for a bit over 10% of the additional code
we have. In this file, you'll find a very important function: `print_hash_table()`. This function has a single task, given a `hash_id`, it will print the structure of the 
hash table to a file. To make things easier, the _format_ it does so is https://graphviz.org/[Graphviz]. You can see typical use case for this function in <<print_hash_table>>.

[source]
[[print_hash_table]]
.Printing the hash table in Graphviz format to file
----
FILE* f = fopen("/tmp/db/hash.graphwiz", "wt");
ensure(print_hash_table(f, &w, hash_id));
fclose(f);
----

You can then take the file generated by <<print_hash_table>> and run `dot -Tsvg /tmp/db/hash.graphwiz > hash.svg`, which will give you an SVG file which you can open. The 
idea is that you'll get a visualization of the hash table as it stands. That can be _invaluable_ to understand exactly what is going on in the system. I have used this 
extensively while I made sure that the hash table does the right thing in all scenarios. Instead of trying to track everything in my head, I had a very visual representation
and very obvious path to understand exactly what is going on here.

Note that for a hash tables with 12,000 entries, we got files in the MB range, far too big to be able to present their image in this format, I'm afraid. This technique is 
something that we'll repeat in other cases in Gavran as needed. In some scenarios, the debug / scaffolding code can be longer than the actual code. But it is worth it to
be able to reduce the troubleshooting time.

And now, for the the actual tests, which you can see in <<tests14>>. You'll note that by now I'm confident enough in our transaction support that I can do all my tests on 
a single transaction, there is nothing to gain from doing commits / separate transactions to read the data. 

[source]
[[tests14]]
.`test.c` - Testing various operations on the extendible hash table
----
include::./code/test.c[tags=tests14]
----